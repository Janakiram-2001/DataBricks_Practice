{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76891e7d-e778-46bb-ae60-65dc894a9c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Important - There are 3 typical read modes and the default read mode is permissive.\n",
    "##### 1. permissive — All fields are set to null and corrupted records are placed in a string column called _corrupt_record\n",
    "##### \t2. dropMalformed — Drops all rows containing corrupt records.\n",
    "##### 3. failFast — Fails when corrupt records are encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bd55059-825c-478c-a385-207db44b9e40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "custom_schema = StructType([StructField('ID', StringType(), True), StructField('Name', StringType(), True), StructField('Age', StringType(), True), StructField('Salary', StringType(), True), StructField('JoinDate', StringType(), True), StructField('LastLogin', StringType(), True), StructField('Notes', StringType(), True),StructField('Corrupted_data', StringType(), True)])\n",
    "\n",
    "df1 = spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/malformed_data.txt\",schema=custom_schema,mode=\"permissive\",comment=\"#\",header=True,multiLine=True,columnNameOfCorruptRecord=\"Corrupted_data\")\n",
    "print(df1.count())\n",
    "display(df1)\n",
    "\n",
    "df2 = spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/malformed_data.txt\",schema=custom_schema,mode=\"dropmalformed\",comment=\"#\",header=True,multiLine=True,columnNameOfCorruptRecord=\"Corrupted_data\")\n",
    "print(df2.count())\n",
    "display(df2)\n",
    "\n",
    "df3 = spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/malformed_data.txt\",schema=custom_schema,mode=\"failfast\",comment=\"#\",header=True,multiLine=True,columnNameOfCorruptRecord=\"Corrupted_data\")\n",
    "print(df3.count())\n",
    "display(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84659783-a48b-4e34-8c60-65066642195d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####CSV Advanced Feature - Very Important, Important, Not Important (just try to know once for [all](url))\n",
    "**Very Important:** path, schema, sep, header, inferSchema, samplingRatio, mode, columnNameOfCorruptRecord <br>\n",
    "**Important:** quote, escape,comment, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, dateFormat, timestampFormat, multiLine, enforceSchema,pathGlobFilter, recursiveFileLookup,modifiedBefore,modifiedAfter<br>\n",
    "**Not Important:** encoding, positiveInf, negativeInf, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, charToEscapeQuoteEscaping, emptyValue, locale, lineSep, unescapedQuoteHandling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815dc56a-1099-4e5b-9cae-7651254127ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "custom_schema = \"cid string, name string,amount string, dop string\"\n",
    "df1 = spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/malformeddata.txt\",schema=custom_schema,comment=\"#\")\n",
    "df1.where(\"upper(amount) == lower(amount)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e8bb5d1-5ecf-4451-805b-664214db4523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#**Very Important:** path, schema, sep, header, inferSchema, samplingRatio, mode, columnNameOfCorruptRecord <br>\n",
    "struct1=\"cid int,name string,amt decimal(10,2),dop date\"\n",
    "\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/we47/we47schema/we47volume/we47directory/malformeddata.txt\",header=False,sep=\",\",comment='#',mode='dropMalformed') #Blindly drop malformed records\n",
    "display(df1)\n",
    "\n",
    "struct1=\"cid int,name string,amt decimal(10,2),dop date,corrupt_record string\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/we47/we47schema/we47volume/we47directory/malformeddata.txt\",header=False,sep=\",\",comment='#',mode='permissive',columnNameOfCorruptRecord='corrupt_record') #permit malformed records and allow me to do a RCA (Root cause analysis)\n",
    "display(df1.where(\"corrupt_record is not null\"))\n",
    "\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/we47/we47schema/we47volume/we47directory/malformeddata.txt\",header=False,sep=\",\",comment='#',mode='failFast')#Fail immediately when malformed records occurs\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be8f4b20-9284-4038-abd1-e728e678a144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#**Important:** quote, escape,comment, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace,  dateFormat, timestampFormat, multiLine, enforceSchema,pathGlobFilter, recursiveFileLookup,modifiedBefore,modifiedAfter, nullValue, nanValue<br>\n",
    "\n",
    "struct1=\"cid int,name string,amt decimal(10,2),dop date\"\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/we47/we47schema/we47volume/we47directory/malformeddata1.txt\",header=False,sep=\",\",comment='#',quote=\"'\",escape='|',ignoreLeadingWhiteSpace=True,ignoreTrailingWhiteSpace=True,dateFormat='yyyy-dd-MM',mode=\"permissive\",modifiedAfter='2025-12-20',multiLine=True,nullValue='na')\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d1a9bce-69a0-4197-b7f8-12abe029b72c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####JSON Advanced Feature - \n",
    "**Very Important** - path,schema,columnNameOfCorruptRecord,dateFormat,timestampFormat,multiLine,pathGlobFilter,recursiveFileLookup<br>\n",
    "No header, No inferSchema, No sep in json...<br>\n",
    "**Important** - primitivesAsString(consider all cols as string), prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, lineSep, samplingRatio, dropFieldIfAllNull, modifiedBefore, modifiedAfter, useUnsafeRow(This is performance optimization when the data is loaded into spark memory) <br>\n",
    "**Not Important** (just try to know once for all) - allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, allowUnquotedControlChars, encoding, locale, allowNonNumericNumbers<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9400d40a-64cf-4282-8c3d-49c290209930",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json=spark.read.json(\"/Volumes/we47/we47schema/we47volume/we47directory/simple_json.txt\",samplingRatio=0.1,prefersDecimal=True)\n",
    "\n",
    "#primitivesAsString\n",
    "# df_json=spark.read.json(\"/Volumes/we47/we47schema/we47volume/we47directory/simple_json.txt\",samplingRatio=0.1,primitivesAsString=True)\n",
    "\n",
    "df_json.printSchema()\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1464b7d-caaa-4c88-a506-d09c3d99edaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema1 = \"amt float,dop date,id int,name string,corrupt_record string\"\n",
    "df1_json = spark.read.json(\"/Volumes/we47/we47schema/we47volume/we47directory/simple_json2.txt\",schema=schema1,mode=\"permissive\",columnNameOfCorruptRecord=\"corrupt_record\",lineSep=\"~\",dateFormat=\"yyyy-dd-MM\",allowComments=True,allowSingleQuotes=True,allowUnquotedFieldNames=True)\n",
    "df1_json.printSchema()\n",
    "df1_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fee04ec-5303-4bc1-ae67-ea210ea8c190",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt1=\"id int,name string,amt float, dop date,custom_corruptrow string\"\n",
    "df_json=spark.read.json(\"/Volumes/we47/we47schema/we47volume/we47directory/simple_json_multiline3.txt\",allowComments=True,allowSingleQuotes=True,allowUnquotedFieldNames=True,columnNameOfCorruptRecord=\"custom_corruptrow\",schema=strt1,dateFormat='yyyy-dd-MM',multiLine=True)\n",
    "#primitivesAsString\n",
    "df_json.printSchema()\n",
    "df_json.show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "345436c6-788f-4686-b133-a69f77218139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Serialized data Advanced Feature - orc, parquet/delta (very very important & we learn indepth)\n",
    "- PathOrPaths\n",
    "- **mergeSchema** - Important interview property (make it proactive/make it driven in the interview) SCHEMA EVOLUTION\n",
    "- pathGlobFilter\n",
    "- recursiveFileLookup\n",
    "- modifiedBefore\n",
    "- modifiedAfter\n",
    "Problem statement:\n",
    "Source is sending data in any way they want...\n",
    "Day1/source1- 5 cols\n",
    "Day2/source2 - 7 Cols\n",
    "\n",
    "1. I am reading the dataframe in csv/json...\n",
    "2. Writing into a orc/parquet format in a single location.\n",
    "3. Reading data in a orc/parquet format using mergeSchema option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee391bd2-8223-4c36-9a4d-8e012715501b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "problem:\n",
    "Source is sending data in any way they want...\n",
    "Day1/source1- 5 cols\n",
    "Day2/source2 - 7 Cols\n",
    "1. I am reading the dataframe in csv/json...\n",
    "2. Writing into a orc/parquet format in a single location.\n",
    "3. Reading data in a orc/parquet format using mergeSchema option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b14a4fe-d349-4215-bdfa-9e3e2704bdd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "orc merge schema"
    }
   },
   "outputs": [],
   "source": [
    "#I am converting the CSV data into ORC data format to achieve SCHEMA EVOLUTION(CHANGING)\n",
    "spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/source1.txt\",inferSchema=True,header=True).write.orc(\"/Volumes/we47/we47schema/we47volume/we47directory/orc_targetdata_merged/\")\n",
    "spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/source2.txt\",inferSchema=True,header=True).write.orc(\"/Volumes/we47/we47schema/we47volume/we47directory/orc_targetdata_merged/\",mode='append')\n",
    "spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/source3.txt\",inferSchema=True,header=True).write.orc(\"/Volumes/we47/we47schema/we47volume/we47directory/orc_targetdata_merged/\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "475ad9c8-a4e6-453c-9174-8964fa1b7680",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orc_data_merged = spark.read.orc(\"/Volumes/we47/we47schema/we47volume/we47directory/orc_targetdata_merged/\",mergeSchema=True)\n",
    "display(orc_data_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16243c3-c01a-4c3a-ab1a-2a7023e4d050",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "parquet merge schema"
    }
   },
   "outputs": [],
   "source": [
    "#I am converting the CSV data into ORC data format to achieve SCHEMA EVOLUTION(CHANGING)\n",
    "spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/source1.txt\",inferSchema=True,header=True).write.parquet(\"/Volumes/we47/we47schema/we47volume/we47directory/parquet_targetdata_merged/\")\n",
    "spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/source2.txt\",inferSchema=True,header=True).write.parquet(\"/Volumes/we47/we47schema/we47volume/we47directory/parquet_targetdata_merged/\",mode='append')\n",
    "spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/source3.txt\",inferSchema=True,header=True).write.parquet(\"/Volumes/we47/we47schema/we47volume/we47directory/parquet_targetdata_merged/\",mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b19b568e-078c-4d11-89c5-9442b67d8cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_data_merged = spark.read.parquet(\"/Volumes/we47/we47schema/we47volume/we47directory/parquet_targetdata_merged/\",mergeSchema=True)\n",
    "display(parquet_data_merged)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "advanced_read_ops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
