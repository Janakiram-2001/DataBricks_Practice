{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e70ff1-6ad5-4b70-a4f7-212697f508c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "plain csv reading without any additional parameters"
    }
   },
   "outputs": [],
   "source": [
    "#If I don't use any options in this csv function, what is the default functionality?\n",
    "#1. By default it will consider ',' as a delimiter (sep='~')\n",
    "#2. By default it will use _c0,_c1..._cn it will apply as column headers (header=True or toDF(\"\",\"\",\"\") or we have more options to see further)\n",
    "#3. By default it will treat all columns as string (inferSchema=True or we have more options to see further)\n",
    "csv_df1=spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df1.printSchema())\n",
    "display(csv_df1)#display with produce output in a beautified table format, specific to databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89f648cd-da49-401d-8171-68d279b07a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "csv_df1=spark.read.csv\n",
    "(path: PathOrPaths, schema: Optional[Union[StructType, str]]=None, sep: Optional[str]=None, encoding: Optional[str]=None, quote: Optional[str]=None, escape: Optional[str]=None, comment: Optional[str]=None, header: Optional[Union[bool, str]]=None, inferSchema: Optional[Union[bool, str]]=None, ignoreLeadingWhiteSpace: Optional[Union[bool, str]]=None, ignoreTrailingWhiteSpace: Optional[Union[bool, str]]=None, nullValue: Optional[str]=None, nanValue: Optional[str]=None, positiveInf: Optional[str]=None, negativeInf: Optional[str]=None, dateFormat: Optional[str]=None, timestampFormat: Optional[str]=None, maxColumns: Optional[Union[int, str]]=None, maxCharsPerColumn: Optional[Union[int, str]]=None, maxMalformedLogPerPartition: Optional[Union[int, str]]=None, mode: Optional[str]=None, columnNameOfCorruptRecord: Optional[str]=None, multiLine: Optional[Union[bool, str]]=None, charToEscapeQuoteEscaping: Optional[str]=None, samplingRatio: Optional[Union[float, str]]=None, enforceSchema: Optional[Union[bool, str]]=None, emptyValue: Optional[str]=None, locale: Optional[str]=None, lineSep: Optional[str]=None, pathGlobFilter: Optional[Union[bool, str]]=None, recursiveFileLookup: Optional[Union[bool, str]]=None, modifiedBefore: Optional[Union[bool, str]]=None, modifiedAfter: Optional[Union[bool, str]]=None, unescapedQuoteHandling: Optional[str]=None) -> \"DataFrame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f36b42-cbc5-4f6f-9266-5d308c0265cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "toDf"
    }
   },
   "outputs": [],
   "source": [
    "#1. Header Concepts (Either we have define the column names or we have to use the column names from the data)\n",
    "#Important option is...\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, but we are asking spark to take the first row as header and not as a data?\n",
    "csv_df1=spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs_header\",header=True)\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "csv_df2=spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df2.printSchema())\n",
    "csv_df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01065290-fc1c-461e-8f2e-6d7180355d59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "inferschema"
    }
   },
   "outputs": [],
   "source": [
    "#3. Inferring Schema \n",
    "# (Performance Consideration: Use this function causiously because it scans the entire data by immediately evaluating and executing\n",
    "# hence, not good for large data or not good to use on the predefined schema dataset)\n",
    "#sample data\n",
    "#4004979,Tara,Drake,32,\n",
    "#4004980,Earl,Hahn,34,Human resources assistant\n",
    "#4004981,Don,Jones,THIRTY SIX,Lawyer\n",
    "csv_df1=spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "csv_df1.where(\"id in (4004979,4004981)\").show(2)\n",
    "csv_df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fda6881-373b-4645-9c19-69fb8e93344c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Performance Importance: Though inferSchema has to be used causiously, we can improve performance by using an option to reduce the data scanned for large data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d2b06c0-0dd8-4175-9cad-1c3697d27ef8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "sampling ratio"
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs\",inferSchema=True,samplingRatio=.10).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\");\n",
    "csv_df1.where(\"id in (4004979,4004981)\").show(2)\n",
    "csv_df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cff90fd0-a082-4a17-a9c3-fd7b77efd6fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "sep ~"
    }
   },
   "outputs": [],
   "source": [
    "csv_df1 = spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs_header_oth_del\",inferSchema=True,header=True,sep=\"~\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\");\n",
    "csv_df1.show(2)\n",
    "csv_df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49173ed6-4c01-46bd-a9fb-df6c035f3bc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. Using different options to create dataframe with csv and other module... (2 methodologies (spark.read.inbuiltfunction or spark.read.format(anyfunction).load(\"path\")) with 3 ways of creating dataframes (pass parameters to the csv()/option/options))\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/we47/we47schema/we47volume/we47directory/custs_header\",inferSchema=True,header=True)\n",
    "csv_df1.show(2)\n",
    "#or another way of creating dataframe (from any sources whether builtin or external)...\n",
    "#option can be used for 1 or 2 option...\n",
    "csv_df2 = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs_header\")\n",
    "csv_df2.show(2)\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "\n",
    "#***if we do not have header row in the csv file and you keep header=true parameter and then if you use toDf function it will override the first row of data as column names.\n",
    "csv_df3 = spark.read.options(header=\"true\",inferSchema=\"true\",sep=\",\").csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b7c1b2c-c943-481f-8bdc-aa4213db08d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Generic way of read and load data into dataframe using fundamental options from built in sources (csv/orc/parquet/xml/json/table) (inferschema, header, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b267a5-bea0-4a61-8325-5cfb58108e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/we47/we47schema/we47volume/we47directory/custs_header\",inferSchema=True,header=True,sep=',')\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02dbedd8-78be-467f-966f-d711d96ede42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Generic way of read and load data into dataframe using extended options from external sources (bigquery/redshift/athena/synapse) (tmpfolder, access controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4bd759-2cd3-4aac-9236-2138df6e8d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df3=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"dbfs:///Volumes/we47/we47schema/we47volume/we47directory/custs_header_oth_del\")\n",
    "csv_df3.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c636c26c-91cf-4581-98a9-705e6453d368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading data from multiple files & Multiple Path (We still have few more options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f32a83fd-81ac-4d70-9082-ce7b40620f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/we47/we47schema/we47volume/we47directory/custs*\",inferSchema=True,header=True,sep='~')\n",
    "print(csv_df1.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4ccaf761-90da-4f9a-ad22-c679d55f7880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1=spark.read.csv(path=[\"/Volumes/we47/we47schema/we47volume/we47directory/custs*\",\"/Volumes/we47/we47schema/we47volume/we47directory/custs\"],inferSchema=True,header=True,sep=',')\n",
    "print(csv_df1.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "698de76d-04d1-484e-a5c0-673f7bd9c90b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Requirement: I am getting data from different source systems of different regions (NY, TX, CA) into different landing pad (locations), how to access this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46d40a7-47ac-4d9e-8b30-aef2b0fa0ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_multiple_sources=spark.read.csv(path=[\"/Volumes/we47/we47schema/we47volume/we47directory/NY/\",\"/Volumes/we47/we47schema/we47volume/we47directory/TX/\"],inferSchema=True,header=True,sep=',',pathGlobFilter=\"custs_head*\",recursiveFileLookup=True)\n",
    "#.toDF(\"cid\",\"fn\",\"ln\",\"a\",\"p\")\n",
    "print(df_multiple_sources.count())\n",
    "df_multiple_sources.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04fa45dd-ef7c-4445-a312-db0ca8072633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Provide schema with SQL String or programatically (very very important)\n",
    "[PySpark SQL Datatypes](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) <br>\n",
    "[Data Types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html)<br>\n",
    "###To provide schema (columname & datatype), what are the 2 basic options available that we learned so far ? inferSchema/toDF<br>\n",
    "###We are going to learn additionally 2 more options to handle schema (colname & datatype)?<br>\n",
    "###1. Using simple string format of define schema.<br>\n",
    "###IMPORTANT: 2. Using structure type to define schema.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a102e042-dff6-4768-be0e-23d41f3f51bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "\n",
    "csv_df1=spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1=spark.read.csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs\",inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df1.printSchema())\n",
    "\n",
    "#1. Using simple string format of define custom simple schema.\n",
    "str_struct=\"id integer,fname string,lname string,age integer,prof string\"\n",
    "csv_df1=spark.read.schema(str_struct).csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs_header\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)\n",
    "\n",
    "#2. Important part - Using structure type to define custom complex schema.\n",
    "#4000001,Kristina,Chung,55,Pilot\n",
    "#pattern - \n",
    "#import the types library based classes..\n",
    "# define_structure=StructType([StructField(\"colname\",DataType(),True),StructField(\"colname\",DataType(),True)...])\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"id\",IntegerType(),False),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"prof\",StringType())])\n",
    "csv_df1=spark.read.schema(custom_schema).csv(\"/Volumes/we47/we47schema/we47volume/we47directory/custs_header\")\n",
    "print(csv_df1.printSchema())\n",
    "csv_df1.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1831dbf5-e913-4fed-af1b-776948fac970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "what will happen if one of the file have an extra column, rest all the columns are same?<br>\n",
    "Is union operation performing here in dataframe? not directly, but we can do to answer above question.. unionByName (Schema evolution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e820227b-77a2-4479-a167-938bdbad24fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What are all the overall options we used in this notebook, for learning fundamental spark csv read operations?\n",
    "1. How to create/manage Catalog & Volume\n",
    "2. spark session\n",
    "3. How to create some sample data and push it to the volume/folder/file\n",
    "4. spark.read.csv operations & spark.read.format().load()\n",
    "5. Few of the important read options under csv such as header, sep, inferSchema, toDF.\n",
    "6. How to define custom schema/structure using mere string with colname & datatype or by using StructType([StructField(colname,DataType()...)]) (very important).\n",
    "7. Few additional options such as samplingRatio, recursiveFileLookup & pathGlobFilter for accessing folders and subfolders with some pattern of filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f7bc06-e5bc-477a-8651-2b83dec390b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What are all the overall options we used in this notebook, for learning fundamental spark dataframe write operations in different formats and targets?\n",
    "1. df.write.csv/json/orc/parquet/table/xml... operations & df.write.format('delta').save()\n",
    "2. Few of the important read options under csv such as header, sep, mode(append/overwrite/error/ignore), toDF.\n",
    "3. Few additional options such as compression, different file formats..."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Read_Ops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
